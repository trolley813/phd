Введем несколько определений, используемых в дальнейшем.
\begin{defn}
    Генеративно-состязательная сеть (GAN) - это нейронная сеть, основной целью которой является генерация объектов,
    сходных с таковыми из заданного набора (по некоторой метрике) \cite{10.5555/2969033.2969125}.
\end{defn}
Это поведение реализуется с помощью следующей архитектуры:
\begin{itemize}
    \item генерирующая сеть $G$ (генератор) создает (генерирует) объекты заданной структуры.
    \item различающая сеть $D$ (дискриминатор) сопоставляет сгенерированные объекты с набором эталонных (ground-truth) значений,
          делая выводы об их сходстве. Сеть G обучается на основе обратной связи, полученной от сети D (с использованием обычных
          методов обратного распространения ошибок).
\end{itemize}
Генеративно-состязательные сети относятся к обучению без учителя. Выставление меток в обучающем наборе не требуется,
единственной требуемой частью является набор эталонных значений.

Следует обратить внимание, что GAN --- это в первую очередь концепция, нежели архитектура, а это означает, что
генеративно-состязательный подход может быть использован с любой сетевой архитектурой (например, многослойными
персептронами~\cite{Rosenblatt1958ThePA}, LSTM-сетями~\cite{10.1162/neco.1997.9.8.1735} и т. д.). В данной работе
концепция GAN используется с многослойными сетями с прямой связью.
Типы сетей, представленные здесь, --- это полностью связанная сеть типа персептрона и сеть, имеющая несколько слоев для отсева
(drop-out).

Введем также несколько понятий из теории информационного поиска, в первую очередь понятия частоты слова и обратной частоты документа.

\begin{defn}
    Метрика $TF$ (term frequency --- частота слова) --- отношение числа вхождений некоторого слова к общему числу слов документа.
\end{defn}
Таким образом, $TF$ служит как бы оценкой важности того или иного слова в пределах конкретного документа.
В качестве формул для вычисления $TF$ чаще всего используются \cite{manning_raghavan_schuetze_2008}
непосредственное количество вхождений слова
\begin{equation}
    \label{eq:raw-tf}
    \text{TF}(w, d) = N_w(d)
\end{equation}
и нормированное количество
\begin{equation}
    \label{eq:norm-tf}
    \text{TF}(w, d) = \frac{N_w(d)}{N(d)},
\end{equation}
где $\text{TF}(w, d)$ --- метрика $TF$ применительно к слову $w$ в документе $d$, $N_w(d)$ --- количество вхождений слова $w$
в документ $d$, а $N(d)$ --- общее количество слов в документе $d$ (иными словами, длина документа $d$, выраженная в словах).

Метрика $IDF$ была впервые введена Карен Спарк Джонс в \cite{jones2004statistical}. Она определяется таким образом:
\begin{defn}
    Метрика IDF (inverse document frequency --- обратная частота документа) --- инверсия частоты, с которой некоторое слово
    встречается в документах коллекции.
\end{defn}
Таким образом, $IDF$ показывает, насколько то или иное слово распространено в документах коллекции. Чем более широко употребительным
является слово, тем меньше его $IDF$. Чаще всего данная метрика используется в связке с $TF$ в качестве весовой.

В большинстве случаев IDF определяется по формуле
\begin{equation}
    \label{eq:idf}
    \text{IDF}(q_i) = \log \frac{N - n(q_i) + 0.5}{n(q_i) + 0.5},
\end{equation}
где $N$ --- общее количество документов в коллекции, а $n(q_i)$ --- количество документов, содержащих слово $q_i$
(данная формула используется в алгоритме BM25 \cite{Amati2009}). Однако, определение \eqref{eq:idf} обладает существенным недостатком:
наиболее употребительные слова, а именно, встречающиеся более чем в половине документов из всей коллекции, будут обладать
отрицательными $IDF$. Существует несколько подходов для устранения этого недостатка. Наиболее простым является введение в формулу
слагаемого сдвига:
\begin{equation}
    \label{eq:shifted-idf}
    \text{IDF}(q_i) = \log \frac{N - n(q_i) + 0.5}{n(q_i) + 0.5} - \log\frac{0.5}{N + 0.5},
\end{equation}
тогда IDF слова, содержащегося во всех без исключения документах коллекции, будет равна нулю, а в противном случае (если есть
хотя бы один документ, не содержащий данного слова) она будет строго положительной.

Перейдем к определениям, касающимся семантического анализа (в первую очередь, из дистрибутивной семантики).
\begin{defn}
    Коэффициент семантической близости, реже семантического подобия (semantic similarity) --- метрика (скаляр), определенная
    на множестве документов, определяющая <<расстояние>> между словами на основании сходства значений этих слов.
\end{defn}

Семантическое подобие может определяться как на основании фактических данных (например, тезауруса --- словаря синонимов),
так и с помощью средств дистрибутивной семантики --- области лингвистики, которая занимается вычислением степени
семантической близости между лингвистическими единицами на основании их распределения (дистрибуции) в больших массивах
лингвистических данных (текстовых корпусах). В настоящей работе рассматриваются оба определения.

\begin{defn}
    Векторное представление слов --- общее название для различных подходов к моделированию языка и обучению представлений
    в обработке естественного языка, направленных на сопоставление словам (и, возможно, фразам) из некоторого словаря
    векторов из $\mathbb{R}^{n}$ для $n$, значительно меньшего количества слов в словаре.
\end{defn}