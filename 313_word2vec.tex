Word2vec --- общее название для совокупности моделей на основе искусственных нейронных сетей, предназначенных для получения
векторных представлений слов на естественном языке. Используется для анализа семантики естественных языков, основанный 
на дистрибутивной семантике, машинном обучении и векторном представлении слов. Программное обеспечение под названием
<<Word2vec>> было разработано группой исследователей Google в 2013 году \cite{mikolov-etal-2013-linguistic,mikolov2013efficient}.
Подобные модели и инструменты разрабатывались и ранее, еще в 2000-х годах (\cite{bengio2003neural,collobert2008}),
однако первой популярной реализацией векторно-семантической модели стала именно Word2vec.

Заметим, что ошибочно называть Word2vec алгоритмом --- в совокупности реализованы два основных алгоритма обучения: 
CBoW (англ. Continuous Bag of Words, <<непрерывный мешок со словами>>, англ. bag — мультимножество) 
и Skip-gram. 

CBoW — архитектура, которая предсказывает текущее слово, исходя из окружающего его контекста. С другой стороны, архитектура
типа Skip-gram действует наоборот: она использует текущее слово, чтобы предугадывать окружающие его слова. 
Построение модели word2vec возможно с помощью двух данных алгоритмов. 
Порядок слов контекста не оказывает влияния на результат ни в одном из этих алгоритмов.

Следует также обратить внимание на то, что word2vec использует в своей работе простые нейронные сети, а это значит,
что для качественных результатов, получаемых при обучении данных сетей, необходимо иметь большой размер обучающей выборки ---
в данном случае, корпуса текстов.