Word2vec --- общее название для совокупности моделей на основе искусственных нейронных сетей, предназначенных для получения
векторных представлений слов на естественном языке. Используется для анализа семантики естественных языков, основанный
на дистрибутивной семантике, машинном обучении и векторном представлении слов. Программное обеспечение под названием
<<Word2vec>> было разработано группой исследователей Google в 2013 году \cite{mikolov-etal-2013-linguistic,mikolov2013efficient}.
Подобные модели и инструменты разрабатывались и ранее, еще в 2000-х годах (\cite{bengio2003neural,collobert2008}),
однако первой популярной реализацией векторно-семантической модели стала именно Word2vec.

Заметим, что ошибочно называть Word2vec алгоритмом --- в совокупности реализованы два основных алгоритма обучения:
CBoW (англ. Continuous Bag of Words, <<непрерывный мешок со словами>>, англ. bag --- мультимножество)
и Skip-gram.

CBoW --- архитектура, которая предсказывает текущее слово, исходя из окружающего его контекста. С другой стороны, архитектура
типа Skip-gram действует наоборот: она использует текущее слово, чтобы предугадывать окружающие его слова.
Построение модели word2vec возможно с помощью двух данных алгоритмов.
Порядок слов контекста не оказывает влияния на результат ни в одном из этих алгоритмов.

Следует также обратить внимание на то, что word2vec использует в своей работе простые нейронные сети, а это значит,
что для качественных результатов, получаемых при обучении данных сетей, необходимо иметь большой размер обучающей выборки ---
в данном случае, корпуса текстов.

В ходе исследования модели, основанной на семантической базе знаний WordNet, описанной в предыдущем разделе, был выявлен 
фундаментальный ее недостаток --- а именно, модель довольно плохо учитывает значение слов именно в контексте области поиска
(поисковой базы данных), поскольку WordNet является базой общего назначения. Кроме того, адекватно определить семантическую 
близость некоторых категорий слов невозможно путем использования только WordNet.

В противоположность вышеописанной, модель с использованием алгоритмов word2vec может обучаться на произвольном наборе текстов,
а это значит, что для обучения мы можем использовать саму поисковую базу (то есть, выборку текстов непосредственно из корпуса).